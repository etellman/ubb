\documentclass[letterpaper, landscape]{exam}
\usepackage{2in1, lscape} 
\printanswers{}

\usepackage{units} 
\usepackage{parskip} 
\usepackage{xfrac} 
\usepackage[fleqn]{amsmath}
\usepackage{cancel}
\usepackage{float}
\usepackage{mdwlist}
\usepackage{booktabs}
\usepackage{cancel}
\usepackage{polynom}
\usepackage{caption}
\usepackage{fullpage}
\usepackage{comment}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{mathtools} 
\usepackage{commath}

\everymath{\displaystyle}

\title{Statistics \\ Chapter 17}
\date{\today}
\author{}

\begin{document}

  \maketitle
  \setcounter{tocdepth}{2}
  \tableofcontents

  \section{Review} % (fold)
  
  \subsection{$\mu$ and $\sigma$ Both Known} % (fold)
  
  When you know $\mu$ and $\sigma$, you can calculate a z-value for a single
  number. This tells you how many standard deviations this value is from the
  mean:
  \[
    z = \frac{x - \mu}{\sigma}
  \]

  \subsection{$\sigma$ Known} % (fold)

  When you know $\sigma$ but not $\mu$ and want to estimate $\mu$, you 
  compute the sample standard deviation from $\sigma$ and $n$:
  \[
    s = \frac{\sigma}{\sqrt{n}}
  \]

  This allows you to find a confidence interval for the actual value of
  $\mu$:
  \[
    \mu = \bar{x} \pm z^* s
  \]

  It also allows you to do a significance test for a hypothesized $\mu$:
  \[
    z = \frac{\bar{x} - \mu_0}{s}
  \]

  You can check in a table to see if $\mu$ is plausible based on your sample.

  \subsection{Neither $\mu$ nor $\sigma$ Known} % (fold)

  When you don't know either $\sigma$ or $\mu$ and want to estimate $\mu$, you calculate the
  standard error ($s$ is the sample standard deviation):
  \[
    se = \frac{s}{\sqrt{n}}
  \]

  This allows you to find a confidence interval for the actual value of
  $\mu$ using t-distribution:
  \[
    \mu = \bar{x} \pm t^* se
  \]

  It also allows you to do a hypothesis test for a hypothesized $\mu$:
  \[
    t = \frac{\bar{x} - \mu_0}{se}
  \]

  You can check in a table to see if $\mu$ is plausible based on your sample.

  \subsection{Matched Pairs} % (fold)

  To do a matched pairs test, you have two sets of related measurements and want
  to estimate the difference between the means. You do this by subtracting each
  pair of measurements and doing a t-test (confidence interval or significance
  test) on the differences.

  \section{Two-Sample Problems} % (fold)
  
  \subsection{Goal}

  Instead of comparing treatment with control, we sometimes want to compare two
  treatments:
  \begin{itemize*}
    \item Does drug A work better than Drug B\@?
    \item Which of two advertising campaigns is more effective?
    \item How much higher is cost of living in Seattle than Denver?
  \end{itemize*}

  \subsection{Mean} % (fold)

  Calculating the mean is easy and doesn't matter how you do it.
  \begin{align*}
    x       & = \{1, 2, 3, \dots 10 \} \\
    y       & = \{10, 9, 8, \dots 1 \} \\
    \\
    \bar{x} & = \bar{y} = 5 \\
    s_x     & = s_y \approx 3.02765 \\
  \end{align*}

  It doesn't matter how we calculate the mean of the difference or difference of
  the means:
  \begin{itemize*}
    \item $\bar{x} - \bar{y}$
    \item subtract individual entries in original order and average
    \item subtract individual entries in some other order and average
  \end{itemize*}

  \subsection{Standard Deviation} % (fold)
  
  Finding the sample standard deviation is harder.

  If original populations have large $\sigma$ there will be large variation
  between the differences between individual values from each population. If
  original populations have small $\sigma$, there will be low variation between
  differences between individual values from each population

  The standard deviation for the differences varies depending on how we do it:
  \begin{itemize}
    \item sort and subtract: $s = 0$. This is the same as subtracting the sample
      standard deviations.

    \item inverse sort and subtract: $s = 2 \cdot s_x \approx 6.055301$. This is
      the same as adding the sample standard deviations.

    \item random shuffle both lists and subtract: $s \approx 4.281744$ 
      
      The results varies depending on chance. Longer lists yield more consistent results.

  \end{itemize}

  The random shuffle standard deviation is the same as finding the distance using Pythagorean
  Theorem between the two sample standard deviations.
  \[
    \sqrt{s_1^2 + s_2^2} \approx \sqrt{2 \cdot 3.02765^2 \approx 4.281744}
  \]

  The same ideas apply when $x$ and $y$ are different lists with different
  means.

  \subsection{Procedure} % (fold)

  \subsubsection{Standard Error} % (fold)

  For two-sample tests, the standard error is given by:
  \begin{align*}
    se &= \sqrt{ se_1^2 + se_2^2 } \\
       & = \sqrt{ \del{ \frac{s_1}{\sqrt{n_1}} }^2 + \del{ \frac{s_2}{\sqrt{n_2}} }^2 } \\
       & =  \sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}} \\
  \end{align*}
  
  \subsubsection{Confidence Interval} % (fold)

  \[
    \mu_1 - \mu_2 = (\bar{x}_1 - \bar{x}_2) \pm t^* \cdot se 
  \]
  
  \subsection{Significance Tests} % (fold)
  
  \[
    t = \frac{(\bar{x}_1 - \bar{x}_2) - \mu_0}{se}
  \]
  
  \section{Examples} % (fold)

  \subsection{Logging (18.5)} % (fold)
  
  \begin{table}[ht]
    \centering
    \begin{tabular}{lrrr}
      \toprule
      Group & n  & mean  & s \\
      \midrule
      L     & 9  & 13.67 & 4.50 \\
      U     & 12 & 17.50 & 3.53 \\
      \bottomrule
    \end{tabular}
    \caption{Exercise 18.5}
  \end{table}

  \begin{enumerate}[(a)]
    \item They might do something different if they knew they were being watched.

    \item We need to do a comparison between logged and unlogged. If the mean
      difference is zero, then logging is harmless.

      \begin{itemize*}
        \item $H_0$: $\mu_L = \mu_U$
        \item $H_a$: $\mu_L < \mu_U$
      \end{itemize*}

      \begin{align*}
        se & \approx 1.8133 \\
        t  & \approx 2.1141 \\
      \end{align*}

      With 8 degrees of freedom and a one-sided test $0.025 < P < 0.05$.

      There is a significant difference, $P < 0.05$ that logging reduces the number
      of trees.

  \end{enumerate}

  \subsection{Exercise 18.7} % (fold)

  \begin{align*}
    se &= 1.8133 \\
    t^* &= 1.860 \\
    \\
    \mu_U - \mu_L & \approx (17.5 - 13.67) \pm 1.860 \cdot 1.8133 \\
                  & \approx 3.83 \pm 3.3727 \\
  \end{align*}
  
  \subsection{Exercise 18.6} % (fold)

  \begin{table}[ht]
    \centering
    \begin{tabular}{lrrr}
      \toprule
      Group & n  & mean   & s \\
      \midrule
      lean  & 10 & 501.65 & 52.04 \\
      obese & 10 & 491.74 & 46.59 \\
      \bottomrule
    \end{tabular}
    \caption{Exercise 18.6}
  \end{table}
  
  \begin{align*}
    se & \approx 22.0898 \\
    t  & \approx 0.4483 \\
  \end{align*}

  With 9 degrees of freedom and a one-sided test $P > 0.5$. There doesn't seem to be
  a difference.

  \subsection{Exercise 18.9} % (fold)

  \begin{table}[ht]
    \centering
    \begin{tabular}{llrrr}
      \toprule
      data set & scent    & n  & mean   & s \\
      \midrule
      money    & none     & 30 & 17.51  & 2.36 \\
      money    & lavender & 30 & 21.12  & 2.35 \\
      time     & none     & 30 & 91.27  & 14.93 \\
      time     & lavender & 30 & 105.70 & 13.10 \\
       \bottomrule
    \end{tabular}
  \end{table}


  
\end{document}

